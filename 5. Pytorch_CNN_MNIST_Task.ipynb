{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o_c8HgUJVq_z",
        "outputId": "6e13a6b1-ddeb-45df-a8e0-f00f23d94e60"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.4.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.4.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.4.0->torchvision) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.0->torchvision) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0PB8l9HuVUsx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ConvNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(ConvNet,self).__init__()\n",
        "    self.cn1=nn.Conv2d(1,16,3,1)\n",
        "    self.cn2=nn.Conv2d(16,32,3,1)\n",
        "    self.dp1=nn.Dropout2d(0.10)\n",
        "    self.dp2 = nn.Dropout2d(0.25)\n",
        "    self.fc1=nn.Linear(4608,64)\n",
        "    self.fc2=nn.Linear(64,10)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.cn1(x)\n",
        "    x=F.relu(x)\n",
        "    x=self.cn2(x)\n",
        "    x=self.dp1(x)\n",
        "    x = F.relu(x)\n",
        "    x = F.max_pool2d(x, 2)\n",
        "    x = self.dp1(x)\n",
        "    x=torch.flatten(x,1)\n",
        "    x=self.fc1(x)\n",
        "    x = F.relu(x)\n",
        "    x = self.dp2(x)\n",
        "    x=self.fc2(x)\n",
        "    op=F.log_softmax(x,dim=1)\n",
        "    return op"
      ],
      "metadata": {
        "id": "f6TfMRugVaoe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model,device,train_dataloader,optim,epoch):\n",
        "  model.train()\n",
        "  for b_i, (X,y) in enumerate(train_dataloader):\n",
        "    X,y=X.to(device),y.to(device)\n",
        "    optim.zero_grad()\n",
        "    pred_prob=model(X)\n",
        "    loss=F.nll_loss(pred_prob,y)\n",
        "    loss.backward()\n",
        "    optim.step()\n",
        "    if b_i%10==0:\n",
        "      print('epoch: {} [{}/{} ({:.0f}%)]\\t training loss: {:.6f}'.format(\n",
        "                epoch, b_i * len(X), len(train_dataloader.dataset),\n",
        "                100. * b_i / len(train_dataloader), loss.item()))\n",
        "\n"
      ],
      "metadata": {
        "id": "kqvBr6woWa4F"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model,device,test_dataloader):\n",
        "  model.eval()\n",
        "  loss=0\n",
        "  success=0\n",
        "  with torch.no_grad():\n",
        "    for X,y in test_dataloader:\n",
        "      X,y = X.to(device), y.to(device)\n",
        "      pred_prob=model(X)\n",
        "      loss+=F.nll_loss(pred_prob,y,reduction='sum').item()\n",
        "      pred=pred_prob.argmax(dim=1,keepdim=True)\n",
        "      success+=pred.eq(y.view_as(pred)).sum().item()\n",
        "  loss /= len(test_dataloader.dataset)\n",
        "\n",
        "  print('\\nTest dataset: Overall Loss: {:.4f}, Overall Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        loss, success, len(test_dataloader.dataset),\n",
        "        100. * success / len(test_dataloader.dataset)))\n"
      ],
      "metadata": {
        "id": "293g1DjsW3ag"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))])), # train_X.mean()/256. and train_X.std()/256.\n",
        "    batch_size=32, shuffle=True)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../data', train=False,\n",
        "                   transform=transforms.Compose([\n",
        "                       transforms.ToTensor(),\n",
        "                       transforms.Normalize((0.1302,), (0.3069,))\n",
        "                   ])),\n",
        "    batch_size=500, shuffle=False)"
      ],
      "metadata": {
        "id": "76XS99ajXhz8"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = ConvNet()\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.5)\n"
      ],
      "metadata": {
        "id": "pQi0H1qgXkYy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(1, 3):\n",
        "    train(model, device, train_dataloader, optimizer, epoch)\n",
        "    test(model, device, test_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq6jcYXtXqqR",
        "outputId": "894c8eb5-fcfe-441f-961f-09b1e4c39c41"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:1374: UserWarning: dropout2d: Received a 2-D input to dropout2d, which is deprecated and will result in an error in a future release. To retain the behavior and silence this warning, please use dropout instead. Note that dropout2d exists to provide channel-wise dropout on inputs with 2 spatial dimensions, a channel dimension, and an optional batch dimension (i.e. 3D or 4D inputs).\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 [0/60000 (0%)]\t training loss: 2.293381\n",
            "epoch: 1 [320/60000 (1%)]\t training loss: 1.816214\n",
            "epoch: 1 [640/60000 (1%)]\t training loss: 1.171479\n",
            "epoch: 1 [960/60000 (2%)]\t training loss: 0.819485\n",
            "epoch: 1 [1280/60000 (2%)]\t training loss: 0.739516\n",
            "epoch: 1 [1600/60000 (3%)]\t training loss: 0.394021\n",
            "epoch: 1 [1920/60000 (3%)]\t training loss: 0.465716\n",
            "epoch: 1 [2240/60000 (4%)]\t training loss: 0.589886\n",
            "epoch: 1 [2560/60000 (4%)]\t training loss: 0.354187\n",
            "epoch: 1 [2880/60000 (5%)]\t training loss: 0.376631\n",
            "epoch: 1 [3200/60000 (5%)]\t training loss: 0.544930\n",
            "epoch: 1 [3520/60000 (6%)]\t training loss: 0.272909\n",
            "epoch: 1 [3840/60000 (6%)]\t training loss: 0.428567\n",
            "epoch: 1 [4160/60000 (7%)]\t training loss: 0.457157\n",
            "epoch: 1 [4480/60000 (7%)]\t training loss: 0.264451\n",
            "epoch: 1 [4800/60000 (8%)]\t training loss: 0.440254\n",
            "epoch: 1 [5120/60000 (9%)]\t training loss: 0.075095\n",
            "epoch: 1 [5440/60000 (9%)]\t training loss: 0.412936\n",
            "epoch: 1 [5760/60000 (10%)]\t training loss: 0.129949\n",
            "epoch: 1 [6080/60000 (10%)]\t training loss: 0.281868\n",
            "epoch: 1 [6400/60000 (11%)]\t training loss: 0.541646\n",
            "epoch: 1 [6720/60000 (11%)]\t training loss: 0.064098\n",
            "epoch: 1 [7040/60000 (12%)]\t training loss: 0.302567\n",
            "epoch: 1 [7360/60000 (12%)]\t training loss: 0.067018\n",
            "epoch: 1 [7680/60000 (13%)]\t training loss: 0.424298\n",
            "epoch: 1 [8000/60000 (13%)]\t training loss: 0.271128\n",
            "epoch: 1 [8320/60000 (14%)]\t training loss: 0.325511\n",
            "epoch: 1 [8640/60000 (14%)]\t training loss: 0.155679\n",
            "epoch: 1 [8960/60000 (15%)]\t training loss: 0.110498\n",
            "epoch: 1 [9280/60000 (15%)]\t training loss: 0.182842\n",
            "epoch: 1 [9600/60000 (16%)]\t training loss: 0.050940\n",
            "epoch: 1 [9920/60000 (17%)]\t training loss: 0.259571\n",
            "epoch: 1 [10240/60000 (17%)]\t training loss: 0.145745\n",
            "epoch: 1 [10560/60000 (18%)]\t training loss: 0.495331\n",
            "epoch: 1 [10880/60000 (18%)]\t training loss: 0.090604\n",
            "epoch: 1 [11200/60000 (19%)]\t training loss: 0.109900\n",
            "epoch: 1 [11520/60000 (19%)]\t training loss: 0.401381\n",
            "epoch: 1 [11840/60000 (20%)]\t training loss: 0.135890\n",
            "epoch: 1 [12160/60000 (20%)]\t training loss: 0.383502\n",
            "epoch: 1 [12480/60000 (21%)]\t training loss: 0.030511\n",
            "epoch: 1 [12800/60000 (21%)]\t training loss: 0.202770\n",
            "epoch: 1 [13120/60000 (22%)]\t training loss: 0.082668\n",
            "epoch: 1 [13440/60000 (22%)]\t training loss: 0.210298\n",
            "epoch: 1 [13760/60000 (23%)]\t training loss: 0.136720\n",
            "epoch: 1 [14080/60000 (23%)]\t training loss: 0.067400\n",
            "epoch: 1 [14400/60000 (24%)]\t training loss: 0.187209\n",
            "epoch: 1 [14720/60000 (25%)]\t training loss: 0.108345\n",
            "epoch: 1 [15040/60000 (25%)]\t training loss: 0.485445\n",
            "epoch: 1 [15360/60000 (26%)]\t training loss: 0.076631\n",
            "epoch: 1 [15680/60000 (26%)]\t training loss: 0.207411\n",
            "epoch: 1 [16000/60000 (27%)]\t training loss: 0.077638\n",
            "epoch: 1 [16320/60000 (27%)]\t training loss: 0.099891\n",
            "epoch: 1 [16640/60000 (28%)]\t training loss: 0.060806\n",
            "epoch: 1 [16960/60000 (28%)]\t training loss: 0.173490\n",
            "epoch: 1 [17280/60000 (29%)]\t training loss: 0.208975\n",
            "epoch: 1 [17600/60000 (29%)]\t training loss: 0.067311\n",
            "epoch: 1 [17920/60000 (30%)]\t training loss: 0.078515\n",
            "epoch: 1 [18240/60000 (30%)]\t training loss: 0.030123\n",
            "epoch: 1 [18560/60000 (31%)]\t training loss: 0.158937\n",
            "epoch: 1 [18880/60000 (31%)]\t training loss: 0.349340\n",
            "epoch: 1 [19200/60000 (32%)]\t training loss: 0.058716\n",
            "epoch: 1 [19520/60000 (33%)]\t training loss: 0.058222\n",
            "epoch: 1 [19840/60000 (33%)]\t training loss: 0.269536\n",
            "epoch: 1 [20160/60000 (34%)]\t training loss: 0.084128\n",
            "epoch: 1 [20480/60000 (34%)]\t training loss: 0.133053\n",
            "epoch: 1 [20800/60000 (35%)]\t training loss: 0.070968\n",
            "epoch: 1 [21120/60000 (35%)]\t training loss: 0.377356\n",
            "epoch: 1 [21440/60000 (36%)]\t training loss: 0.124650\n",
            "epoch: 1 [21760/60000 (36%)]\t training loss: 0.124132\n",
            "epoch: 1 [22080/60000 (37%)]\t training loss: 0.166997\n",
            "epoch: 1 [22400/60000 (37%)]\t training loss: 0.170113\n",
            "epoch: 1 [22720/60000 (38%)]\t training loss: 0.551292\n",
            "epoch: 1 [23040/60000 (38%)]\t training loss: 0.027386\n",
            "epoch: 1 [23360/60000 (39%)]\t training loss: 0.254295\n",
            "epoch: 1 [23680/60000 (39%)]\t training loss: 0.139361\n",
            "epoch: 1 [24000/60000 (40%)]\t training loss: 0.041631\n",
            "epoch: 1 [24320/60000 (41%)]\t training loss: 0.066858\n",
            "epoch: 1 [24640/60000 (41%)]\t training loss: 0.054633\n",
            "epoch: 1 [24960/60000 (42%)]\t training loss: 0.049180\n",
            "epoch: 1 [25280/60000 (42%)]\t training loss: 0.025732\n",
            "epoch: 1 [25600/60000 (43%)]\t training loss: 0.225129\n",
            "epoch: 1 [25920/60000 (43%)]\t training loss: 0.552562\n",
            "epoch: 1 [26240/60000 (44%)]\t training loss: 0.102813\n",
            "epoch: 1 [26560/60000 (44%)]\t training loss: 0.026151\n",
            "epoch: 1 [26880/60000 (45%)]\t training loss: 0.126202\n",
            "epoch: 1 [27200/60000 (45%)]\t training loss: 0.049303\n",
            "epoch: 1 [27520/60000 (46%)]\t training loss: 0.048586\n",
            "epoch: 1 [27840/60000 (46%)]\t training loss: 0.142810\n",
            "epoch: 1 [28160/60000 (47%)]\t training loss: 0.013890\n",
            "epoch: 1 [28480/60000 (47%)]\t training loss: 0.498274\n",
            "epoch: 1 [28800/60000 (48%)]\t training loss: 0.082913\n",
            "epoch: 1 [29120/60000 (49%)]\t training loss: 0.310890\n",
            "epoch: 1 [29440/60000 (49%)]\t training loss: 0.041700\n",
            "epoch: 1 [29760/60000 (50%)]\t training loss: 0.053140\n",
            "epoch: 1 [30080/60000 (50%)]\t training loss: 0.556713\n",
            "epoch: 1 [30400/60000 (51%)]\t training loss: 0.195235\n",
            "epoch: 1 [30720/60000 (51%)]\t training loss: 0.076892\n",
            "epoch: 1 [31040/60000 (52%)]\t training loss: 0.188041\n",
            "epoch: 1 [31360/60000 (52%)]\t training loss: 0.054415\n",
            "epoch: 1 [31680/60000 (53%)]\t training loss: 0.039152\n",
            "epoch: 1 [32000/60000 (53%)]\t training loss: 0.010883\n",
            "epoch: 1 [32320/60000 (54%)]\t training loss: 0.061184\n",
            "epoch: 1 [32640/60000 (54%)]\t training loss: 0.362655\n",
            "epoch: 1 [32960/60000 (55%)]\t training loss: 0.013326\n",
            "epoch: 1 [33280/60000 (55%)]\t training loss: 0.409972\n",
            "epoch: 1 [33600/60000 (56%)]\t training loss: 0.131231\n",
            "epoch: 1 [33920/60000 (57%)]\t training loss: 0.118389\n",
            "epoch: 1 [34240/60000 (57%)]\t training loss: 0.033827\n",
            "epoch: 1 [34560/60000 (58%)]\t training loss: 0.001596\n",
            "epoch: 1 [34880/60000 (58%)]\t training loss: 0.416056\n",
            "epoch: 1 [35200/60000 (59%)]\t training loss: 0.427084\n",
            "epoch: 1 [35520/60000 (59%)]\t training loss: 0.022024\n",
            "epoch: 1 [35840/60000 (60%)]\t training loss: 0.085094\n",
            "epoch: 1 [36160/60000 (60%)]\t training loss: 0.015156\n",
            "epoch: 1 [36480/60000 (61%)]\t training loss: 0.137712\n",
            "epoch: 1 [36800/60000 (61%)]\t training loss: 0.064697\n",
            "epoch: 1 [37120/60000 (62%)]\t training loss: 0.104300\n",
            "epoch: 1 [37440/60000 (62%)]\t training loss: 0.181633\n",
            "epoch: 1 [37760/60000 (63%)]\t training loss: 0.020358\n",
            "epoch: 1 [38080/60000 (63%)]\t training loss: 0.110308\n",
            "epoch: 1 [38400/60000 (64%)]\t training loss: 0.062820\n",
            "epoch: 1 [38720/60000 (65%)]\t training loss: 0.355213\n",
            "epoch: 1 [39040/60000 (65%)]\t training loss: 0.085426\n",
            "epoch: 1 [39360/60000 (66%)]\t training loss: 0.275043\n",
            "epoch: 1 [39680/60000 (66%)]\t training loss: 0.037531\n",
            "epoch: 1 [40000/60000 (67%)]\t training loss: 0.084768\n",
            "epoch: 1 [40320/60000 (67%)]\t training loss: 0.073673\n",
            "epoch: 1 [40640/60000 (68%)]\t training loss: 0.238090\n",
            "epoch: 1 [40960/60000 (68%)]\t training loss: 0.281005\n",
            "epoch: 1 [41280/60000 (69%)]\t training loss: 0.143574\n",
            "epoch: 1 [41600/60000 (69%)]\t training loss: 0.233417\n",
            "epoch: 1 [41920/60000 (70%)]\t training loss: 0.053566\n",
            "epoch: 1 [42240/60000 (70%)]\t training loss: 0.029468\n",
            "epoch: 1 [42560/60000 (71%)]\t training loss: 0.056274\n",
            "epoch: 1 [42880/60000 (71%)]\t training loss: 0.189057\n",
            "epoch: 1 [43200/60000 (72%)]\t training loss: 0.043981\n",
            "epoch: 1 [43520/60000 (73%)]\t training loss: 0.269865\n",
            "epoch: 1 [43840/60000 (73%)]\t training loss: 0.084407\n",
            "epoch: 1 [44160/60000 (74%)]\t training loss: 0.156429\n",
            "epoch: 1 [44480/60000 (74%)]\t training loss: 0.115468\n",
            "epoch: 1 [44800/60000 (75%)]\t training loss: 0.175993\n",
            "epoch: 1 [45120/60000 (75%)]\t training loss: 0.125667\n",
            "epoch: 1 [45440/60000 (76%)]\t training loss: 0.180016\n",
            "epoch: 1 [45760/60000 (76%)]\t training loss: 0.014562\n",
            "epoch: 1 [46080/60000 (77%)]\t training loss: 0.260316\n",
            "epoch: 1 [46400/60000 (77%)]\t training loss: 0.308914\n",
            "epoch: 1 [46720/60000 (78%)]\t training loss: 0.124445\n",
            "epoch: 1 [47040/60000 (78%)]\t training loss: 0.054389\n",
            "epoch: 1 [47360/60000 (79%)]\t training loss: 0.098756\n",
            "epoch: 1 [47680/60000 (79%)]\t training loss: 0.093874\n",
            "epoch: 1 [48000/60000 (80%)]\t training loss: 0.048526\n",
            "epoch: 1 [48320/60000 (81%)]\t training loss: 0.151325\n",
            "epoch: 1 [48640/60000 (81%)]\t training loss: 0.013944\n",
            "epoch: 1 [48960/60000 (82%)]\t training loss: 0.005442\n",
            "epoch: 1 [49280/60000 (82%)]\t training loss: 0.162784\n",
            "epoch: 1 [49600/60000 (83%)]\t training loss: 0.016296\n",
            "epoch: 1 [49920/60000 (83%)]\t training loss: 0.067168\n",
            "epoch: 1 [50240/60000 (84%)]\t training loss: 0.126766\n",
            "epoch: 1 [50560/60000 (84%)]\t training loss: 0.009736\n",
            "epoch: 1 [50880/60000 (85%)]\t training loss: 0.019350\n",
            "epoch: 1 [51200/60000 (85%)]\t training loss: 0.176550\n",
            "epoch: 1 [51520/60000 (86%)]\t training loss: 0.017976\n",
            "epoch: 1 [51840/60000 (86%)]\t training loss: 0.048894\n",
            "epoch: 1 [52160/60000 (87%)]\t training loss: 0.056682\n",
            "epoch: 1 [52480/60000 (87%)]\t training loss: 0.080693\n",
            "epoch: 1 [52800/60000 (88%)]\t training loss: 0.015468\n",
            "epoch: 1 [53120/60000 (89%)]\t training loss: 0.010163\n",
            "epoch: 1 [53440/60000 (89%)]\t training loss: 0.020732\n",
            "epoch: 1 [53760/60000 (90%)]\t training loss: 0.033104\n",
            "epoch: 1 [54080/60000 (90%)]\t training loss: 0.006229\n",
            "epoch: 1 [54400/60000 (91%)]\t training loss: 0.074558\n",
            "epoch: 1 [54720/60000 (91%)]\t training loss: 0.029893\n",
            "epoch: 1 [55040/60000 (92%)]\t training loss: 0.006162\n",
            "epoch: 1 [55360/60000 (92%)]\t training loss: 0.028341\n",
            "epoch: 1 [55680/60000 (93%)]\t training loss: 0.070194\n",
            "epoch: 1 [56000/60000 (93%)]\t training loss: 0.214220\n",
            "epoch: 1 [56320/60000 (94%)]\t training loss: 0.212750\n",
            "epoch: 1 [56640/60000 (94%)]\t training loss: 0.103612\n",
            "epoch: 1 [56960/60000 (95%)]\t training loss: 0.046912\n",
            "epoch: 1 [57280/60000 (95%)]\t training loss: 0.075190\n",
            "epoch: 1 [57600/60000 (96%)]\t training loss: 0.020675\n",
            "epoch: 1 [57920/60000 (97%)]\t training loss: 0.061061\n",
            "epoch: 1 [58240/60000 (97%)]\t training loss: 0.039944\n",
            "epoch: 1 [58560/60000 (98%)]\t training loss: 0.023345\n",
            "epoch: 1 [58880/60000 (98%)]\t training loss: 0.030526\n",
            "epoch: 1 [59200/60000 (99%)]\t training loss: 0.014004\n",
            "epoch: 1 [59520/60000 (99%)]\t training loss: 0.023567\n",
            "epoch: 1 [59840/60000 (100%)]\t training loss: 0.009654\n",
            "\n",
            "Test dataset: Overall Loss: 0.0505, Overall Accuracy: 9833/10000 (98%)\n",
            "\n",
            "epoch: 2 [0/60000 (0%)]\t training loss: 0.046483\n",
            "epoch: 2 [320/60000 (1%)]\t training loss: 0.216207\n",
            "epoch: 2 [640/60000 (1%)]\t training loss: 0.055800\n",
            "epoch: 2 [960/60000 (2%)]\t training loss: 0.149805\n",
            "epoch: 2 [1280/60000 (2%)]\t training loss: 0.018699\n",
            "epoch: 2 [1600/60000 (3%)]\t training loss: 0.045112\n",
            "epoch: 2 [1920/60000 (3%)]\t training loss: 0.007097\n",
            "epoch: 2 [2240/60000 (4%)]\t training loss: 0.053935\n",
            "epoch: 2 [2560/60000 (4%)]\t training loss: 0.045111\n",
            "epoch: 2 [2880/60000 (5%)]\t training loss: 0.035966\n",
            "epoch: 2 [3200/60000 (5%)]\t training loss: 0.081708\n",
            "epoch: 2 [3520/60000 (6%)]\t training loss: 0.047832\n",
            "epoch: 2 [3840/60000 (6%)]\t training loss: 0.061615\n",
            "epoch: 2 [4160/60000 (7%)]\t training loss: 0.059960\n",
            "epoch: 2 [4480/60000 (7%)]\t training loss: 0.094139\n",
            "epoch: 2 [4800/60000 (8%)]\t training loss: 0.017256\n",
            "epoch: 2 [5120/60000 (9%)]\t training loss: 0.198796\n",
            "epoch: 2 [5440/60000 (9%)]\t training loss: 0.008261\n",
            "epoch: 2 [5760/60000 (10%)]\t training loss: 0.022051\n",
            "epoch: 2 [6080/60000 (10%)]\t training loss: 0.013864\n",
            "epoch: 2 [6400/60000 (11%)]\t training loss: 0.114983\n",
            "epoch: 2 [6720/60000 (11%)]\t training loss: 0.324574\n",
            "epoch: 2 [7040/60000 (12%)]\t training loss: 0.018572\n",
            "epoch: 2 [7360/60000 (12%)]\t training loss: 0.030023\n",
            "epoch: 2 [7680/60000 (13%)]\t training loss: 0.027760\n",
            "epoch: 2 [8000/60000 (13%)]\t training loss: 0.063761\n",
            "epoch: 2 [8320/60000 (14%)]\t training loss: 0.025746\n",
            "epoch: 2 [8640/60000 (14%)]\t training loss: 0.019277\n",
            "epoch: 2 [8960/60000 (15%)]\t training loss: 0.018269\n",
            "epoch: 2 [9280/60000 (15%)]\t training loss: 0.087733\n",
            "epoch: 2 [9600/60000 (16%)]\t training loss: 0.019908\n",
            "epoch: 2 [9920/60000 (17%)]\t training loss: 0.006051\n",
            "epoch: 2 [10240/60000 (17%)]\t training loss: 0.043641\n",
            "epoch: 2 [10560/60000 (18%)]\t training loss: 0.147337\n",
            "epoch: 2 [10880/60000 (18%)]\t training loss: 0.109727\n",
            "epoch: 2 [11200/60000 (19%)]\t training loss: 0.126049\n",
            "epoch: 2 [11520/60000 (19%)]\t training loss: 0.187442\n",
            "epoch: 2 [11840/60000 (20%)]\t training loss: 0.063730\n",
            "epoch: 2 [12160/60000 (20%)]\t training loss: 0.042148\n",
            "epoch: 2 [12480/60000 (21%)]\t training loss: 0.021684\n",
            "epoch: 2 [12800/60000 (21%)]\t training loss: 0.016472\n",
            "epoch: 2 [13120/60000 (22%)]\t training loss: 0.345423\n",
            "epoch: 2 [13440/60000 (22%)]\t training loss: 0.013458\n",
            "epoch: 2 [13760/60000 (23%)]\t training loss: 0.008706\n",
            "epoch: 2 [14080/60000 (23%)]\t training loss: 0.010699\n",
            "epoch: 2 [14400/60000 (24%)]\t training loss: 0.236392\n",
            "epoch: 2 [14720/60000 (25%)]\t training loss: 0.006991\n",
            "epoch: 2 [15040/60000 (25%)]\t training loss: 0.302897\n",
            "epoch: 2 [15360/60000 (26%)]\t training loss: 0.003662\n",
            "epoch: 2 [15680/60000 (26%)]\t training loss: 0.159534\n",
            "epoch: 2 [16000/60000 (27%)]\t training loss: 0.039434\n",
            "epoch: 2 [16320/60000 (27%)]\t training loss: 0.032396\n",
            "epoch: 2 [16640/60000 (28%)]\t training loss: 0.028740\n",
            "epoch: 2 [16960/60000 (28%)]\t training loss: 0.003420\n",
            "epoch: 2 [17280/60000 (29%)]\t training loss: 0.098240\n",
            "epoch: 2 [17600/60000 (29%)]\t training loss: 0.006136\n",
            "epoch: 2 [17920/60000 (30%)]\t training loss: 0.053365\n",
            "epoch: 2 [18240/60000 (30%)]\t training loss: 0.207883\n",
            "epoch: 2 [18560/60000 (31%)]\t training loss: 0.018168\n",
            "epoch: 2 [18880/60000 (31%)]\t training loss: 0.056482\n",
            "epoch: 2 [19200/60000 (32%)]\t training loss: 0.101552\n",
            "epoch: 2 [19520/60000 (33%)]\t training loss: 0.005247\n",
            "epoch: 2 [19840/60000 (33%)]\t training loss: 0.132007\n",
            "epoch: 2 [20160/60000 (34%)]\t training loss: 0.126906\n",
            "epoch: 2 [20480/60000 (34%)]\t training loss: 0.181851\n",
            "epoch: 2 [20800/60000 (35%)]\t training loss: 0.039025\n",
            "epoch: 2 [21120/60000 (35%)]\t training loss: 0.042109\n",
            "epoch: 2 [21440/60000 (36%)]\t training loss: 0.018310\n",
            "epoch: 2 [21760/60000 (36%)]\t training loss: 0.070864\n",
            "epoch: 2 [22080/60000 (37%)]\t training loss: 0.044864\n",
            "epoch: 2 [22400/60000 (37%)]\t training loss: 0.018263\n",
            "epoch: 2 [22720/60000 (38%)]\t training loss: 0.083880\n",
            "epoch: 2 [23040/60000 (38%)]\t training loss: 0.021396\n",
            "epoch: 2 [23360/60000 (39%)]\t training loss: 0.028549\n",
            "epoch: 2 [23680/60000 (39%)]\t training loss: 0.040848\n",
            "epoch: 2 [24000/60000 (40%)]\t training loss: 0.018312\n",
            "epoch: 2 [24320/60000 (41%)]\t training loss: 0.053804\n",
            "epoch: 2 [24640/60000 (41%)]\t training loss: 0.008351\n",
            "epoch: 2 [24960/60000 (42%)]\t training loss: 0.035099\n",
            "epoch: 2 [25280/60000 (42%)]\t training loss: 0.005653\n",
            "epoch: 2 [25600/60000 (43%)]\t training loss: 0.035020\n",
            "epoch: 2 [25920/60000 (43%)]\t training loss: 0.101981\n",
            "epoch: 2 [26240/60000 (44%)]\t training loss: 0.087060\n",
            "epoch: 2 [26560/60000 (44%)]\t training loss: 0.089585\n",
            "epoch: 2 [26880/60000 (45%)]\t training loss: 0.204403\n",
            "epoch: 2 [27200/60000 (45%)]\t training loss: 0.027826\n",
            "epoch: 2 [27520/60000 (46%)]\t training loss: 0.084237\n",
            "epoch: 2 [27840/60000 (46%)]\t training loss: 0.062837\n",
            "epoch: 2 [28160/60000 (47%)]\t training loss: 0.002641\n",
            "epoch: 2 [28480/60000 (47%)]\t training loss: 0.012923\n",
            "epoch: 2 [28800/60000 (48%)]\t training loss: 0.035872\n",
            "epoch: 2 [29120/60000 (49%)]\t training loss: 0.199345\n",
            "epoch: 2 [29440/60000 (49%)]\t training loss: 0.002614\n",
            "epoch: 2 [29760/60000 (50%)]\t training loss: 0.015506\n",
            "epoch: 2 [30080/60000 (50%)]\t training loss: 0.030511\n",
            "epoch: 2 [30400/60000 (51%)]\t training loss: 0.004113\n",
            "epoch: 2 [30720/60000 (51%)]\t training loss: 0.315733\n",
            "epoch: 2 [31040/60000 (52%)]\t training loss: 0.177510\n",
            "epoch: 2 [31360/60000 (52%)]\t training loss: 0.011231\n",
            "epoch: 2 [31680/60000 (53%)]\t training loss: 0.011102\n",
            "epoch: 2 [32000/60000 (53%)]\t training loss: 0.001901\n",
            "epoch: 2 [32320/60000 (54%)]\t training loss: 0.060980\n",
            "epoch: 2 [32640/60000 (54%)]\t training loss: 0.013265\n",
            "epoch: 2 [32960/60000 (55%)]\t training loss: 0.052685\n",
            "epoch: 2 [33280/60000 (55%)]\t training loss: 0.096543\n",
            "epoch: 2 [33600/60000 (56%)]\t training loss: 0.073262\n",
            "epoch: 2 [33920/60000 (57%)]\t training loss: 0.090881\n",
            "epoch: 2 [34240/60000 (57%)]\t training loss: 0.150272\n",
            "epoch: 2 [34560/60000 (58%)]\t training loss: 0.108279\n",
            "epoch: 2 [34880/60000 (58%)]\t training loss: 0.069238\n",
            "epoch: 2 [35200/60000 (59%)]\t training loss: 0.014089\n",
            "epoch: 2 [35520/60000 (59%)]\t training loss: 0.062848\n",
            "epoch: 2 [35840/60000 (60%)]\t training loss: 0.220245\n",
            "epoch: 2 [36160/60000 (60%)]\t training loss: 0.012723\n",
            "epoch: 2 [36480/60000 (61%)]\t training loss: 0.016344\n",
            "epoch: 2 [36800/60000 (61%)]\t training loss: 0.057069\n",
            "epoch: 2 [37120/60000 (62%)]\t training loss: 0.161833\n",
            "epoch: 2 [37440/60000 (62%)]\t training loss: 0.027502\n",
            "epoch: 2 [37760/60000 (63%)]\t training loss: 0.089403\n",
            "epoch: 2 [38080/60000 (63%)]\t training loss: 0.006613\n",
            "epoch: 2 [38400/60000 (64%)]\t training loss: 0.005043\n",
            "epoch: 2 [38720/60000 (65%)]\t training loss: 0.091299\n",
            "epoch: 2 [39040/60000 (65%)]\t training loss: 0.016051\n",
            "epoch: 2 [39360/60000 (66%)]\t training loss: 0.016758\n",
            "epoch: 2 [39680/60000 (66%)]\t training loss: 0.166655\n",
            "epoch: 2 [40000/60000 (67%)]\t training loss: 0.062455\n",
            "epoch: 2 [40320/60000 (67%)]\t training loss: 0.035470\n",
            "epoch: 2 [40640/60000 (68%)]\t training loss: 0.047575\n",
            "epoch: 2 [40960/60000 (68%)]\t training loss: 0.021822\n",
            "epoch: 2 [41280/60000 (69%)]\t training loss: 0.064046\n",
            "epoch: 2 [41600/60000 (69%)]\t training loss: 0.081285\n",
            "epoch: 2 [41920/60000 (70%)]\t training loss: 0.133954\n",
            "epoch: 2 [42240/60000 (70%)]\t training loss: 0.021574\n",
            "epoch: 2 [42560/60000 (71%)]\t training loss: 0.004905\n",
            "epoch: 2 [42880/60000 (71%)]\t training loss: 0.152641\n",
            "epoch: 2 [43200/60000 (72%)]\t training loss: 0.004381\n",
            "epoch: 2 [43520/60000 (73%)]\t training loss: 0.122922\n",
            "epoch: 2 [43840/60000 (73%)]\t training loss: 0.125800\n",
            "epoch: 2 [44160/60000 (74%)]\t training loss: 0.001956\n",
            "epoch: 2 [44480/60000 (74%)]\t training loss: 0.123701\n",
            "epoch: 2 [44800/60000 (75%)]\t training loss: 0.003461\n",
            "epoch: 2 [45120/60000 (75%)]\t training loss: 0.080617\n",
            "epoch: 2 [45440/60000 (76%)]\t training loss: 0.124165\n",
            "epoch: 2 [45760/60000 (76%)]\t training loss: 0.083155\n",
            "epoch: 2 [46080/60000 (77%)]\t training loss: 0.014638\n",
            "epoch: 2 [46400/60000 (77%)]\t training loss: 0.671591\n",
            "epoch: 2 [46720/60000 (78%)]\t training loss: 0.003927\n",
            "epoch: 2 [47040/60000 (78%)]\t training loss: 0.018479\n",
            "epoch: 2 [47360/60000 (79%)]\t training loss: 0.024919\n",
            "epoch: 2 [47680/60000 (79%)]\t training loss: 0.051458\n",
            "epoch: 2 [48000/60000 (80%)]\t training loss: 0.014344\n",
            "epoch: 2 [48320/60000 (81%)]\t training loss: 0.004711\n",
            "epoch: 2 [48640/60000 (81%)]\t training loss: 0.002821\n",
            "epoch: 2 [48960/60000 (82%)]\t training loss: 0.086562\n",
            "epoch: 2 [49280/60000 (82%)]\t training loss: 0.015709\n",
            "epoch: 2 [49600/60000 (83%)]\t training loss: 0.057404\n",
            "epoch: 2 [49920/60000 (83%)]\t training loss: 0.012832\n",
            "epoch: 2 [50240/60000 (84%)]\t training loss: 0.004167\n",
            "epoch: 2 [50560/60000 (84%)]\t training loss: 0.000513\n",
            "epoch: 2 [50880/60000 (85%)]\t training loss: 0.015481\n",
            "epoch: 2 [51200/60000 (85%)]\t training loss: 0.338079\n",
            "epoch: 2 [51520/60000 (86%)]\t training loss: 0.049079\n",
            "epoch: 2 [51840/60000 (86%)]\t training loss: 0.044740\n",
            "epoch: 2 [52160/60000 (87%)]\t training loss: 0.036285\n",
            "epoch: 2 [52480/60000 (87%)]\t training loss: 0.017491\n",
            "epoch: 2 [52800/60000 (88%)]\t training loss: 0.207232\n",
            "epoch: 2 [53120/60000 (89%)]\t training loss: 0.026434\n",
            "epoch: 2 [53440/60000 (89%)]\t training loss: 0.027230\n",
            "epoch: 2 [53760/60000 (90%)]\t training loss: 0.014616\n",
            "epoch: 2 [54080/60000 (90%)]\t training loss: 0.093773\n",
            "epoch: 2 [54400/60000 (91%)]\t training loss: 0.007571\n",
            "epoch: 2 [54720/60000 (91%)]\t training loss: 0.044243\n",
            "epoch: 2 [55040/60000 (92%)]\t training loss: 0.152945\n",
            "epoch: 2 [55360/60000 (92%)]\t training loss: 0.165959\n",
            "epoch: 2 [55680/60000 (93%)]\t training loss: 0.068350\n",
            "epoch: 2 [56000/60000 (93%)]\t training loss: 0.063818\n",
            "epoch: 2 [56320/60000 (94%)]\t training loss: 0.170524\n",
            "epoch: 2 [56640/60000 (94%)]\t training loss: 0.573917\n",
            "epoch: 2 [56960/60000 (95%)]\t training loss: 0.008464\n",
            "epoch: 2 [57280/60000 (95%)]\t training loss: 0.045135\n",
            "epoch: 2 [57600/60000 (96%)]\t training loss: 0.006176\n",
            "epoch: 2 [57920/60000 (97%)]\t training loss: 0.007960\n",
            "epoch: 2 [58240/60000 (97%)]\t training loss: 0.004887\n",
            "epoch: 2 [58560/60000 (98%)]\t training loss: 0.032514\n",
            "epoch: 2 [58880/60000 (98%)]\t training loss: 0.025379\n",
            "epoch: 2 [59200/60000 (99%)]\t training loss: 0.318154\n",
            "epoch: 2 [59520/60000 (99%)]\t training loss: 0.052648\n",
            "epoch: 2 [59840/60000 (100%)]\t training loss: 0.014300\n",
            "\n",
            "Test dataset: Overall Loss: 0.0440, Overall Accuracy: 9857/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_samples = enumerate(test_dataloader)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "b_i, (sample_data, sample_targets) = next(test_samples)\n",
        "\n",
        "plt.imshow(sample_data[0][0], cmap='gray', interpolation='none')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "wDfJyw4DXzpD",
        "outputId": "a8755882-c784-4f46-f243-81eae3977c44"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Model prediction is : {model(sample_data).data.max(1)[1][0]}\")\n",
        "print(f\"Ground truth is : {sample_targets[0]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvLTPxZdYe2o",
        "outputId": "d66a0cac-22f4-49f5-8c3c-3ab8800eebd6"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model prediction is : 7\n",
            "Ground truth is : 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CabeBYsQYghK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}